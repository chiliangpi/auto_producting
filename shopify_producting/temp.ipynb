{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T10:31:26.767409Z",
     "start_time": "2023-09-14T10:31:26.309461Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from paddleocr import PPStructure,draw_structure_result,save_structure_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:36:33.302121Z",
     "start_time": "2023-09-14T10:36:33.259050Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "#Role\n",
    "å•†å“ä¿¡æ¯é‡‡é›†å¸ˆ\n",
    "#Background\n",
    "æˆ‘ä¸ºä½ æä¾›ä¸€äº›å•†å“ä¿¡æ¯ï¼Œä½†æ˜¯æ ¼å¼ä¸è§„èŒƒï¼Œè¯·å¸®æˆ‘æå–å•†å“çš„ä¸»è¦ä¿¡æ¯ï¼Œå¹¶è§„èŒƒåŒ–è¾“å‡º\n",
    "#Workflows\n",
    "--æå–å•†å“åŸå§‹æ ‡é¢˜original_titleï¼Œä¸è¦æœ‰ä»»ä½•åˆ å‡å’Œä¿®æ”¹ï¼›\n",
    "--å•†å“ä¿¡æ¯æ¦‚è¿°descriptionï¼Œè¦æ±‚ä¿¡æ¯å®Œæ•´ä¸”ç®€æ˜æ‰¼è¦ï¼Œçªå‡ºå…³é”®è¯ï¼Œå­—æ•°å°½é‡æ§åˆ¶åœ¨30-50ä¸ªä¹‹é—´ï¼›\n",
    "--æ¦‚æ‹¬ä¸€ä¸ªç®€çŸ­ä¸”ä¸ªæ€§åŒ–çš„æ ‡é¢˜brief_titleï¼Œä¸è¶…è¿‡10ä¸ªå•è¯ï¼›\n",
    "--æ€»ç»“SEOæ ‡é¢˜seo_titleï¼Œä¸è¶…è¿‡30ä¸ªå•è¯ï¼›\n",
    "--æ¨æ–­è°·æ­Œå•†å“ç±»åˆ«google_categoryï¼›\n",
    "--æ¨æ–­shopifyå•†å“ç±»åˆ«shopify_categoryï¼›\n",
    "--æ¨æ–­å•†å“ç±»å‹typeï¼›\n",
    "--æ¨æ–­å•†å“æ ‡ç­¾tagsï¼Œæ•°é‡ä¸è¶…è¿‡5ä¸ªï¼Œä½¿ç”¨é€—å·åˆ†éš”ï¼›\n",
    "--æ¨æ–­å—ä¼—ç¾¤ä½“æ€§åˆ«genderï¼Œåœ¨maleï¼Œfemaleï¼Œunisexä¸­é€‰æ‹©;\n",
    "--æ¨æ–­å—ä¼—ç¾¤ä½“å¹´é¾„age_groupï¼Œåœ¨childï¼Œadultä¸­é€‰æ‹©;\n",
    "--æ¨æ–­å¹¿å‘Šè¯åˆ†ç»„adword_groupï¼Œæ•°é‡ä¸è¶…è¿‡3ä¸ªï¼Œé€—å·åˆ†éš”;\n",
    "--æ¨æ–­å¹¿å‘Šè¯æ ‡ç­¾adword_labelsï¼Œæ•°é‡ä¸è¶…è¿‡5ä¸ªï¼Œé€—å·åˆ†éš”ï¼›\n",
    "--æå–å•†å“MPNç¼–å·mpn_codeï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä¸ºâ€™';\n",
    "--æå–skuä¿¡æ¯sku_detailsï¼ŒåŒ…æ‹¬skuå±æ€§åˆ†ç±»ã€å±æ€§å†…å®¹ã€å¯¹åº”çš„ä»·æ ¼ï¼Œæ ¼å¼ä¸ºåµŒå¥—Jsonï¼Œå¦‚æœæŸå±æ€§æ²¡æœ‰ä»·æ ¼ï¼Œåˆ™ä»·æ ¼ä¸º''ï¼›\n",
    "--æå–å•†å“çš„æœ€é«˜ä»·æ ¼highest_priceï¼Œå¦‚æœåªå­˜åœ¨ä¸€ä¸ªä»·æ ¼ï¼Œåªè¾“å‡ºè¯¥ä»·æ ¼ï¼Œå¦‚æœå­˜åœ¨å¤šä¸ªä»·æ ¼ï¼Œå–æœ€é«˜SKUä»·æ ¼ï¼›\n",
    "--æå–å•†å“çš„ä¸»è¦å±æ€§ä¿¡æ¯main_propertiesï¼ŒåŒ…æ‹¬æè´¨materialã€å·¥è‰ºcraftsmanshipï¼Œæµè¡Œå…ƒç´ fashion_elementã€é£æ ¼styleã€å°ºå¯¸sizeã€å•ä½é‡é‡unit_weightï¼Œæ ¼å¼ä¸ºJsonï¼›\n",
    "--å°†æå–çš„ä¿¡æ¯ä»¥Jsonæ ¼å¼è¾“å‡ºï¼Œåˆ†åˆ«ä¸ºä¸­æ–‡ç‰ˆå’Œè‹±æ–‡ç‰ˆä¸¤ä¸ªJson\n",
    "#Constrains\n",
    "--æå–çš„ä¿¡æ¯ä¸­ä¸èƒ½å«æœ‰è·¨å¢ƒã€ä»£å‘ã€æ‰¹å‘ã€ä¸­å›½åˆ¶é€ çš„ç›¸å…³è¯æ±‡ï¼ˆoriginal_titleå­—æ®µé™¤å¤–ï¼‰\n",
    "--googleå’Œshopifyå•†å“ç±»åˆ«æ˜¯å…·æœ‰å±‚çº§å…³ç³»çš„å®Œæ•´ç±»ç›®\n",
    "--ä»·æ ¼ã€é‡é‡çš„æ ¼å¼ä¸ºæ•°å­—+å•ä½ï¼Œä¾‹å¦‚5å…ƒã€5Kgï¼Œå¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™ç”¨ç©ºå­—ç¬¦â€™â€™æ›¿ä»£ï¼›ä»·æ ¼ä¸éœ€è¦å¸ç§å’Œæ±‡ç‡è½¬æ¢\n",
    "--åªè¾“å‡ºä¸­æ–‡ç‰ˆJsonå’Œè‹±æ–‡ç‰ˆJsonå³å¯ï¼Œä¸éœ€è¦å…¶ä»–ä¿¡æ¯è¾“å‡º\n",
    "#Example\n",
    "è¾“å‡ºJsonæ ¼å¼ç¤ºä¾‹ï¼š{\"original_title\": â€œÃ—Ã—\", \"description\": \"Ã—Ã—\", \"brief_title\": \"Ã—Ã—â€, \"seo_title\": \"Ã—Ã—â€,  \"google_category\": \"Ã—Ã—\", â€œshopify_category\": \"Ã—Ã—\", \"type\": â€œÃ—Ã—â€, \"tags\": â€œÃ—Ã—,Ã—Ã—,Ã—Ã—â€,  \"gender\": â€œÃ—Ã—â€,  \"age_group\": â€œÃ—Ã—â€,  \"adword_group\": â€œÃ—Ã—â€, \"adword_labels\": â€œÃ—Ã—â€, \"mpn_code\": â€œÃ—Ã—â€, \"sku_details\": {â€œå±æ€§åç§°A\": { â€œå±æ€§1\": \"ä»·æ ¼\", â€œå±æ€§2\": \"ä»·æ ¼\"}, \"å±æ€§åç§°B\": { â€œå±æ€§1\": â€˜ä»·æ ¼', â€œå±æ€§2\": â€œä»·æ ¼\"} }, \"highest_price\": â€œä»·æ ¼\", \"main_properties\": { \"material\": \"Ã—Ã—Ã—\", \"craftsmanship\": \"Ã—Ã—Ã—\", \"fashion_element\": \"Ã—Ã—Ã—\", \"style\": \"Ã—Ã—Ã—\", \"size\": â€Ã—Ã—Ã—â€, \"unit_weight\": â€Ã—Ã—Ã—â€}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "RUMNVNTY ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾æ¬§ç¾æ½®æµå˜»å“ˆINSé£æƒ…äººèŠ‚ç¤¼ç‰©ç§äººå®š\n",
    "\n",
    "0æ¡è¯„ä»·\n",
    "90å¤©å†…\n",
    "300+\n",
    "PCSæˆäº¤\n",
    "ä¸¾æŠ¥\n",
    "æ‰¹å‘\n",
    "ä¸€ä»¶ä»£å‘\n",
    "\n",
    "ä»·æ ¼\n",
    "èµ·æ‰¹é‡\n",
    "Â¥\n",
    "6.50\n",
    "~\n",
    "Â¥\n",
    "8.00\n",
    "2PCSèµ·æ‰¹\n",
    "ä¼˜æƒ \n",
    "æ»¡199å…ƒåŒ…é‚®\n",
    "1ä»¶æ··æ‰¹\n",
    "æŸ¥çœ‹\n",
    "æœåŠ¡\n",
    "æé€Ÿé€€æ¬¾\n",
    "7å¤©åŒ…æ¢\n",
    "æè´¨ä¿éšœ\n",
    "æ™šå‘å¿…èµ”\n",
    "ç‰©æµ\n",
    "æµ™æ±Ÿé‡‘å\n",
    "è‡³\n",
    "è¯·é€‰æ‹©\n",
    "è¿è´¹ : é€‰æ‹©æ”¶è´§åœ°\n",
    "æ‰¿è¯º72å°æ—¶å‘è´§\n",
    "æ›´å¤š\n",
    "é¢œè‰²\n",
    "é’¢è‰²\n",
    "6.50å…ƒ\n",
    "9573PCSå¯å”®\n",
    "0\n",
    " \t \n",
    "18Kç‚‰å†…çœŸé‡‘\n",
    "8.00å…ƒ\n",
    "9540PCSå¯å”®\n",
    "0\n",
    " \t \n",
    "ç‚‰å†…ç«ç‘°é‡‘\n",
    "8.00å…ƒ\n",
    "9876PCSå¯å”®\n",
    "0\n",
    " \t \n",
    "é»‘è‰²\n",
    "8.00å…ƒ\n",
    "976PCSå¯å”®\n",
    "è·¨å¢ƒå±æ€§\n",
    "è·¨å¢ƒåŒ…è£¹é‡é‡\n",
    "0.08kg\n",
    "å•ä½é‡é‡\n",
    "0.08kg\n",
    "å•†å“å±æ€§\n",
    "æè´¨\n",
    "é’›é’¢\n",
    "å¤„ç†å·¥è‰º\n",
    "ç”µé•€\n",
    "å“ç‰Œ\n",
    "RUMNVNTY\n",
    "æ ·å¼\n",
    "ç”·å¥³é€šç”¨\n",
    "é€ å‹\n",
    "å¿ƒå½¢\n",
    "é€‚ç”¨é€ç¤¼åœºåˆ\n",
    "æ—…æ¸¸çºªå¿µ\n",
    "è´§å·\n",
    "DH042\n",
    "äº§åœ°\n",
    "å¹¿å·\n",
    "ä¸»è¦ä¸‹æ¸¸å¹³å°\n",
    "æ·˜å®,äº¬ä¸œ,ebay,PDD,äºšé©¬é€Š,wish,å¿«æ‰‹,LINIO,SHEIN,é€Ÿå–é€š,å¤©çŒ«,ç‹¬ç«‹ç«™,LAZADA,æ‹¼å¤šå¤š,shopee,æŠ–éŸ³,å‚ç±»ç”µå•†\n",
    "é¢œè‰²\n",
    "é’¢è‰²,18Kç‚‰å†…çœŸé‡‘,ç‚‰å†…ç«ç‘°é‡‘,é»‘è‰²\n",
    "æœ‰å¯æˆæƒçš„è‡ªæœ‰å“ç‰Œ\n",
    "æ˜¯\n",
    "æ˜¯å¦è·¨å¢ƒå‡ºå£ä¸“ä¾›è´§æº\n",
    "æ˜¯\n",
    "æµè¡Œå…ƒç´ \n",
    "æ¤­åœ†\n",
    "ä¸Šå¸‚å¹´ä»½/å­£èŠ‚\n",
    "2022å¹´å†¬å­£\n",
    "é£æ ¼\n",
    "ä¸ªæ€§\n",
    "é£æ ¼åˆ†ç±»\n",
    "ä¸ªæ€§é£æ½®\n",
    "ä¸»è¦é”€å”®åœ°åŒº\n",
    "éæ´²,æ¬§æ´²,å—ç¾,ä¸œå—äºš,åŒ—ç¾,ä¸œåŒ—äºš,ä¸­ä¸œ,å…¶ä»–\n",
    "é€‚ç”¨äººç¾¤\n",
    "æƒ…ä¾£å¼\n",
    "æµè¡Œå…ƒç´ åˆ†ç±»\n",
    "å‡ ä½•\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7rHDWq8lbfYgJJXC8jsqVCSv3MfNb at 0x7feca80b3e30> JSON: {\n",
       "  \"id\": \"chatcmpl-7rHDWq8lbfYgJJXC8jsqVCSv3MfNb\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1692931922,\n",
       "  \"model\": \"gpt-4-0314\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"{\\n  \\\"original_title\\\": \\\"RUMNVNTY \\u4e0d\\u9508\\u94a2\\u60c5\\u4fa3\\u523b\\u5b57\\u624b\\u94fe\\u6b27\\u7f8e\\u6f6e\\u6d41\\u563b\\u54c8INS\\u98ce\\u60c5\\u4eba\\u8282\\u793c\\u7269\\u79c1\\u4eba\\u5b9a\\\",\\n  \\\"description\\\": \\\"\\u4e0d\\u9508\\u94a2\\u60c5\\u4fa3\\u523b\\u5b57\\u624b\\u94fe\\uff0c\\u6b27\\u7f8e\\u6f6e\\u6d41\\u563b\\u54c8INS\\u98ce\\uff0c\\u9002\\u5408\\u60c5\\u4eba\\u8282\\u793c\\u7269\\uff0c\\u79c1\\u4eba\\u5b9a\\u5236\\\",\\n  \\\"brief_title\\\": \\\"\\u4e0d\\u9508\\u94a2\\u60c5\\u4fa3\\u523b\\u5b57\\u624b\\u94fe\\\",\\n  \\\"seo_title\\\": \\\"RUMNVNTY - \\u4e0d\\u9508\\u94a2\\u60c5\\u4fa3\\u523b\\u5b57\\u624b\\u94fe, \\u6b27\\u7f8e\\u6f6e\\u6d41\\u563b\\u54c8INS\\u98ce, \\u9002\\u7528\\u4e8e\\u60c5\\u4eba\\u8282\\u793c\\u7269\\\",\\n  \\\"google_category\\\": \\\"\\u9996\\u9970\\u4e0e\\u914d\\u4ef6 > \\u624b\\u94fe\\\",\\n  \\\"shopify_category\\\": \\\"\\u9996\\u9970\\u4e0e\\u914d\\u4ef6 > \\u624b\\u94fe\\\",\\n  \\\"type\\\": \\\"\\u624b\\u94fe\\\",\\n  \\\"tags\\\": \\\"\\u4e0d\\u9508\\u94a2,\\u60c5\\u4fa3,\\u523b\\u5b57,\\u624b\\u94fe,\\u793c\\u7269\\\",\\n  \\\"gender\\\": \\\"unisex\\\",\\n  \\\"age_group\\\": \\\"adult\\\",\\n  \\\"adword_group\\\": \\\"\\u624b\\u94fe,\\u60c5\\u4fa3,\\u793c\\u7269\\\",\\n  \\\"adword_labels\\\": \\\"\\u4e0d\\u9508\\u94a2,\\u523b\\u5b57,\\u60c5\\u4eba\\u8282,\\u6b27\\u7f8e\\u6f6e\\u6d41,\\u79c1\\u4eba\\u5b9a\\u5236\\\",\\n  \\\"mpn_code\\\": \\\"DH042\\\",\\n  \\\"sku_details\\\": {\\n    \\\"\\u989c\\u8272\\\": {\\n      \\\"\\u94a2\\u8272\\\": \\\"6.50\\u5143\\\",\\n      \\\"18K\\u7089\\u5185\\u771f\\u91d1\\\": \\\"8.00\\u5143\\\",\\n      \\\"\\u7089\\u5185\\u73ab\\u7470\\u91d1\\\": \\\"8.00\\u5143\\\",\\n      \\\"\\u9ed1\\u8272\\\": \\\"8.00\\u5143\\\"\\n    }\\n  },\\n  \\\"highest_price\\\": \\\"8.00\\u5143\\\",\\n  \\\"main_properties\\\": {\\n    \\\"material\\\": \\\"\\u949b\\u94a2\\\",\\n    \\\"craftsmanship\\\": \\\"\\u7535\\u9540\\\",\\n    \\\"fashion_element\\\": \\\"\\u692d\\u5706\\\",\\n    \\\"style\\\": \\\"\\u4e2a\\u6027\\u98ce\\u6f6e\\\",\\n    \\\"size\\\": \\\"\\\",\\n    \\\"unit_weight\\\": \\\"0.08kg\\\"\\n  }\\n}\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 1497,\n",
       "    \"completion_tokens\": 480,\n",
       "    \"total_tokens\": 1977\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = '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'\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4-0314\",\n",
    "#   max_tokens=100,\n",
    "#   temperature=0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"{system}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/bv/1ly9grbd6w143ngs2zd7406h0000gq/T/ipykernel_15741/2834172697.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m     {\n\u001B[1;32m     29\u001B[0m         \u001B[0;34m\"role\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"assistant\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0;34m\"content\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnull\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m         \"function_call\": {\n\u001B[1;32m     32\u001B[0m             \u001B[0;34m\"name\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"calculate_sum\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "openai.api_key = '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'\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4-0314\",\n",
    "#   max_tokens=100,\n",
    "#   temperature=0,\n",
    "  messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"è¯·å¸®æˆ‘è®¡ç®—5å’Œ7çš„å’Œã€‚\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": None,\n",
    "        \"function_call\": {\n",
    "            \"name\": \"calculate_sum\",\n",
    "            \"arguments\": \"{\\\"a\\\": 5, \\\"b\\\": 7}\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7rH5ApdQfdfhyAbQwLUGwdXPgUix8 at 0x7feca80c4c50> JSON: {\n",
       "  \"id\": \"chatcmpl-7rH5ApdQfdfhyAbQwLUGwdXPgUix8\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1692931404,\n",
       "  \"model\": \"gpt-4-0314\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"5\\u548c7\\u7684\\u548c\\u662f12\\u3002\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 40,\n",
       "    \"completion_tokens\": 8,\n",
       "    \"total_tokens\": 48\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_title': 'rumnvnty ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾æ¬§ç¾æ½®æµå˜»å“ˆinsé£æƒ…äººèŠ‚ç¤¼ç‰©ç§äººå®š',\n",
       " 'description': 'ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾ï¼Œæ¬§ç¾æ½®æµå˜»å“ˆinsé£ï¼Œé€‚åˆæƒ…äººèŠ‚ç¤¼ç‰©ï¼Œç§äººå®šåˆ¶',\n",
       " 'brief_title': 'ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾',\n",
       " 'seo_title': 'rumnvnty - ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾, æ¬§ç¾æ½®æµå˜»å“ˆinsé£, é€‚ç”¨äºæƒ…äººèŠ‚ç¤¼ç‰©',\n",
       " 'google_category': 'é¦–é¥°ä¸é…ä»¶ > æ‰‹é“¾',\n",
       " 'shopify_category': 'é¦–é¥°ä¸é…ä»¶ > æ‰‹é“¾',\n",
       " 'type': 'æ‰‹é“¾',\n",
       " 'tags': 'ä¸é”ˆé’¢,æƒ…ä¾£,åˆ»å­—,æ‰‹é“¾,ç¤¼ç‰©',\n",
       " 'gender': 'unisex',\n",
       " 'age_group': 'adult',\n",
       " 'adword_group': 'æ‰‹é“¾,æƒ…ä¾£,ç¤¼ç‰©',\n",
       " 'adword_labels': 'ä¸é”ˆé’¢,åˆ»å­—,æƒ…äººèŠ‚,æ¬§ç¾æ½®æµ,ç§äººå®šåˆ¶',\n",
       " 'mpn_code': 'dh042',\n",
       " 'sku_details': {'é¢œè‰²': {'é’¢è‰²': '6.50å…ƒ',\n",
       "   '18kç‚‰å†…çœŸé‡‘': '8.00å…ƒ',\n",
       "   'ç‚‰å†…ç«ç‘°é‡‘': '8.00å…ƒ',\n",
       "   'é»‘è‰²': '8.00å…ƒ'}},\n",
       " 'highest_price': '8.00å…ƒ',\n",
       " 'main_properties': {'material': 'é’›é’¢',\n",
       "  'craftsmanship': 'ç”µé•€',\n",
       "  'fashion_element': 'æ¤­åœ†',\n",
       "  'style': 'ä¸ªæ€§é£æ½®',\n",
       "  'size': '',\n",
       "  'unit_weight': '0.08kg'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_str = '{\\n  \\\"original_title\\\": \\\"rumnvnty ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾æ¬§ç¾æ½®æµå˜»å“ˆinsé£æƒ…äººèŠ‚ç¤¼ç‰©ç§äººå®š\\\",\\n  \\\"description\\\": \\\"ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾ï¼Œæ¬§ç¾æ½®æµå˜»å“ˆinsé£ï¼Œé€‚åˆæƒ…äººèŠ‚ç¤¼ç‰©ï¼Œç§äººå®šåˆ¶\\\",\\n  \\\"brief_title\\\": \\\"ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾\\\",\\n  \\\"seo_title\\\": \\\"rumnvnty - ä¸é”ˆé’¢æƒ…ä¾£åˆ»å­—æ‰‹é“¾, æ¬§ç¾æ½®æµå˜»å“ˆinsé£, é€‚ç”¨äºæƒ…äººèŠ‚ç¤¼ç‰©\\\",\\n  \\\"google_category\\\": \\\"é¦–é¥°ä¸é…ä»¶ > æ‰‹é“¾\\\",\\n  \\\"shopify_category\\\": \\\"é¦–é¥°ä¸é…ä»¶ > æ‰‹é“¾\\\",\\n  \\\"type\\\": \\\"æ‰‹é“¾\\\",\\n  \\\"tags\\\": \\\"ä¸é”ˆé’¢,æƒ…ä¾£,åˆ»å­—,æ‰‹é“¾,ç¤¼ç‰©\\\",\\n  \\\"gender\\\": \\\"unisex\\\",\\n  \\\"age_group\\\": \\\"adult\\\",\\n  \\\"adword_group\\\": \\\"æ‰‹é“¾,æƒ…ä¾£,ç¤¼ç‰©\\\",\\n  \\\"adword_labels\\\": \\\"ä¸é”ˆé’¢,åˆ»å­—,æƒ…äººèŠ‚,æ¬§ç¾æ½®æµ,ç§äººå®šåˆ¶\\\",\\n  \\\"mpn_code\\\": \\\"dh042\\\",\\n  \\\"sku_details\\\": {\\n    \\\"é¢œè‰²\\\": {\\n      \\\"é’¢è‰²\\\": \\\"6.50å…ƒ\\\",\\n      \\\"18kç‚‰å†…çœŸé‡‘\\\": \\\"8.00å…ƒ\\\",\\n      \\\"ç‚‰å†…ç«ç‘°é‡‘\\\": \\\"8.00å…ƒ\\\",\\n      \\\"é»‘è‰²\\\": \\\"8.00å…ƒ\\\"\\n    }\\n  },\\n  \\\"highest_price\\\": \\\"8.00å…ƒ\\\",\\n  \\\"main_properties\\\": {\\n    \\\"material\\\": \\\"é’›é’¢\\\",\\n    \\\"craftsmanship\\\": \\\"ç”µé•€\\\",\\n    \\\"fashion_element\\\": \\\"æ¤­åœ†\\\",\\n    \\\"style\\\": \\\"ä¸ªæ€§é£æ½®\\\",\\n    \\\"size\\\": \\\"\\\",\\n    \\\"unit_weight\\\": \\\"0.08kg\\\"\\n  }\\n}'\n",
    "json.loads(js_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getenv in module os:\n",
      "\n",
      "getenv(key, default=None)\n",
      "    Get an environment variable, return None if it doesn't exist.\n",
      "    The optional second argument can specify an alternate default.\n",
      "    key, default and the result are str.\n"
     ]
    }
   ],
   "source": [
    "help(os.getenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/fanke.chang/Downloads/id_cluster_precision.csv', sep='\\t', dtype=str).fillna('')\n",
    "df['admin_division_1'] = df.query_address_component.apply(lambda x: json.loads(x).get('admin_division_1'))\n",
    "df['admin_division_2'] = df.query_address_component.apply(lambda x: json.loads(x).get('admin_division_2'))\n",
    "df['admin_division_3'] = df.query_address_component.apply(lambda x: json.loads(x).get('admin_division_3'))\n",
    "df['query_raw_address'] = df.query_address_component.apply(lambda x: json.loads(x).get('raw_address'))\n",
    "df['recall_raw_address'] = df.recall_address_component.apply(lambda x: json.loads(x).get('raw_address'))\n",
    "df.to_csv('/Users/fanke.chang/Downloads/id_cluster_precision_res.csv', sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Sulawesi Selatan\n",
       "1       Sulawesi Selatan\n",
       "2       Sulawesi Selatan\n",
       "3       Sulawesi Selatan\n",
       "4       Sulawesi Selatan\n",
       "              ...       \n",
       "1171          Jawa Barat\n",
       "1172          Jawa Barat\n",
       "1173          Jawa Barat\n",
       "1174          Jawa Barat\n",
       "1175          Jawa Barat\n",
       "Name: admin_division_1, Length: 1176, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['admin_division_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shopify\n",
    "SHOP_URL = '34460e.myshopify.com'\n",
    "API_KEY = '093a3e097bc79f63fe13c683022bc5e2'\n",
    "PASSWORD = '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'\n",
    "# PASSWORD = 'FaW826204'\n",
    "# shop_url = \"https://%s:%s@%s.myshopify.com/admin\" % (API_KEY, PASSWORD, SHOP_NAME)\n",
    "shopify.ShopifyResource.set_site(f\"https://{API_KEY}:{PASSWORD}@{SHOP_URL}/admin\")\n",
    "# shopify.ShopifyResource.set_site(shop_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID: 8911564308785\n",
      "Product Title: 2023æ—¶å°šæ½®äººå¤åŸƒåŠæ³•è€å¤´åƒå˜»å“ˆåŠå  ä¸ªæ€§å¤å¤ç”·å£«åˆé‡‘é¡¹é“¾æ‰¹å‘\n",
      "------\n",
      "Product ID: 8911596683569\n",
      "Product Title: European and American Hip-hop Trendsetters 13mm Flat Bottomed Cuban Chain with Diamond Bracelets for Men and Women Hiphop Gold Plated Necklace Cross-border\n",
      "------\n",
      "Product ID: 8911565455665\n",
      "Product Title: å„¿ç«¥æ–¹å‘ç›˜æ¨¡æ‹Ÿé©¾é©¶ç©å…·ç›Šæ™ºç”µåŠ¨æ¡Œé¢æ¸¸æˆæœºèº²é¿èµ›è½¦é—¯å…³å¤§å†’é™©\n",
      "------\n",
      "Product ID: 8911596224817\n",
      "Product Title: æ¬§ç¾è·¨å¢ƒæ–°æ¬¾é¦–é¥°çˆ±å¿ƒé•¶é’»é”†çŸ³é¡¹é“¾ç®€çº¦ä¸ªæ€§æ—¶å°šç™¾æ­è€³é¥°å‚å®¶æ‰¹å‘\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "products = shopify.Product.find()\n",
    "for product in products:\n",
    "    print(f\"Product ID: {product.id}\")\n",
    "    print(f\"Product Title: {product.title}\")\n",
    "    # print(f\"Product Body: {product.body_html}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3032645996.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/bv/1ly9grbd6w143ngs2zd7406h0000gq/T/ipykernel_15741/3032645996.py\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    GET https://34460e.myshopify.com/admin/oauth/authorize\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "GET https://34460e.myshopify.com/admin/oauth/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: remote control, remote\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "url = 'https://cdn.shopify.com/s/files/1/0824/1273/2721/files/O1CN014RuCnn1lkYW4Ju12V__2211718304857-0-cib.jpg?v=1693838171'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ViTImageProcessor in module transformers.models.vit.image_processing_vit object:\n",
      "\n",
      "class ViTImageProcessor(transformers.image_processing_utils.BaseImageProcessor)\n",
      " |  ViTImageProcessor(do_resize: bool = True, size: Union[Dict[str, int], NoneType] = None, resample: PIL.Image.Resampling = <Resampling.BILINEAR: 2>, do_rescale: bool = True, rescale_factor: Union[int, float] = 0.00392156862745098, do_normalize: bool = True, image_mean: Union[float, List[float], NoneType] = None, image_std: Union[float, List[float], NoneType] = None, **kwargs) -> None\n",
      " |  \n",
      " |  Constructs a ViT image processor.\n",
      " |  \n",
      " |  Args:\n",
      " |      do_resize (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to resize the image's (height, width) dimensions to the specified `(size[\"height\"],\n",
      " |          size[\"width\"])`. Can be overridden by the `do_resize` parameter in the `preprocess` method.\n",
      " |      size (`dict`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n",
      " |          Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n",
      " |          method.\n",
      " |      resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n",
      " |          Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n",
      " |          `preprocess` method.\n",
      " |      do_rescale (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`\n",
      " |          parameter in the `preprocess` method.\n",
      " |      rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n",
      " |          Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n",
      " |          `preprocess` method.\n",
      " |      do_normalize (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n",
      " |          method.\n",
      " |      image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n",
      " |          Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n",
      " |          channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n",
      " |      image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n",
      " |          Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n",
      " |          number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ViTImageProcessor\n",
      " |      transformers.image_processing_utils.BaseImageProcessor\n",
      " |      transformers.image_processing_utils.ImageProcessingMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, do_resize: bool = True, size: Union[Dict[str, int], NoneType] = None, resample: PIL.Image.Resampling = <Resampling.BILINEAR: 2>, do_rescale: bool = True, rescale_factor: Union[int, float] = 0.00392156862745098, do_normalize: bool = True, image_mean: Union[float, List[float], NoneType] = None, image_std: Union[float, List[float], NoneType] = None, **kwargs) -> None\n",
      " |      Set elements of `kwargs` as attributes.\n",
      " |  \n",
      " |  normalize(self, image: numpy.ndarray, mean: Union[float, List[float]], std: Union[float, List[float]], data_format: Union[transformers.image_utils.ChannelDimension, str, NoneType] = None, **kwargs) -> numpy.ndarray\n",
      " |      Normalize an image. image = (image - image_mean) / image_std.\n",
      " |      \n",
      " |      Args:\n",
      " |          image (`np.ndarray`):\n",
      " |              Image to normalize.\n",
      " |          mean (`float` or `List[float]`):\n",
      " |              Image mean to use for normalization.\n",
      " |          std (`float` or `List[float]`):\n",
      " |              Image standard deviation to use for normalization.\n",
      " |          data_format (`str` or `ChannelDimension`, *optional*):\n",
      " |              The channel dimension format for the output image. If unset, the channel dimension format of the input\n",
      " |              image is used. Can be one of:\n",
      " |              - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
      " |              - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `np.ndarray`: The normalized image.\n",
      " |  \n",
      " |  preprocess(self, images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), List[ForwardRef('PIL.Image.Image')], List[numpy.ndarray], List[ForwardRef('torch.Tensor')]], do_resize: Union[bool, NoneType] = None, size: Dict[str, int] = None, resample: PIL.Image.Resampling = None, do_rescale: Union[bool, NoneType] = None, rescale_factor: Union[float, NoneType] = None, do_normalize: Union[bool, NoneType] = None, image_mean: Union[float, List[float], NoneType] = None, image_std: Union[float, List[float], NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, data_format: Union[str, transformers.image_utils.ChannelDimension] = <ChannelDimension.FIRST: 'channels_first'>, **kwargs)\n",
      " |      Preprocess an image or batch of images.\n",
      " |      \n",
      " |      Args:\n",
      " |          images (`ImageInput`):\n",
      " |              Image to preprocess.\n",
      " |          do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n",
      " |              Whether to resize the image.\n",
      " |          size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n",
      " |              Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n",
      " |              resizing.\n",
      " |          resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n",
      " |              `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`. Only has\n",
      " |              an effect if `do_resize` is set to `True`.\n",
      " |          do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n",
      " |              Whether to rescale the image values between [0 - 1].\n",
      " |          rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n",
      " |              Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n",
      " |          do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n",
      " |              Whether to normalize the image.\n",
      " |          image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n",
      " |              Image mean to use if `do_normalize` is set to `True`.\n",
      " |          image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n",
      " |              Image standard deviation to use if `do_normalize` is set to `True`.\n",
      " |          return_tensors (`str` or `TensorType`, *optional*):\n",
      " |              The type of tensors to return. Can be one of:\n",
      " |              - Unset: Return a list of `np.ndarray`.\n",
      " |              - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n",
      " |              - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n",
      " |              - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n",
      " |              - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n",
      " |          data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n",
      " |              The channel dimension format for the output image. Can be one of:\n",
      " |              - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
      " |              - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
      " |              - Unset: Use the channel dimension format of the input image.\n",
      " |  \n",
      " |  rescale(self, image: numpy.ndarray, scale: float, data_format: Union[transformers.image_utils.ChannelDimension, str, NoneType] = None, **kwargs) -> numpy.ndarray\n",
      " |      Rescale an image by a scale factor. image = image * scale.\n",
      " |      \n",
      " |      Args:\n",
      " |          image (`np.ndarray`):\n",
      " |              Image to rescale.\n",
      " |          scale (`float`):\n",
      " |              The scaling factor to rescale pixel values by.\n",
      " |          data_format (`str` or `ChannelDimension`, *optional*):\n",
      " |              The channel dimension format for the output image. If unset, the channel dimension format of the input\n",
      " |              image is used. Can be one of:\n",
      " |              - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
      " |              - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `np.ndarray`: The rescaled image.\n",
      " |  \n",
      " |  resize(self, image: numpy.ndarray, size: Dict[str, int], resample: PIL.Image.Resampling = <Resampling.BILINEAR: 2>, data_format: Union[transformers.image_utils.ChannelDimension, str, NoneType] = None, **kwargs) -> numpy.ndarray\n",
      " |      Resize an image to `(size[\"height\"], size[\"width\"])`.\n",
      " |      \n",
      " |      Args:\n",
      " |          image (`np.ndarray`):\n",
      " |              Image to resize.\n",
      " |          size (`Dict[str, int]`):\n",
      " |              Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n",
      " |          resample:\n",
      " |              `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.\n",
      " |          data_format (`ChannelDimension` or `str`, *optional*):\n",
      " |              The channel dimension format for the output image. If unset, the channel dimension format of the input\n",
      " |              image is used. Can be one of:\n",
      " |              - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n",
      " |              - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `np.ndarray`: The resized image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  model_input_names = ['pixel_values']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.image_processing_utils.BaseImageProcessor:\n",
      " |  \n",
      " |  __call__(self, images, **kwargs) -> transformers.image_processing_utils.BatchFeature\n",
      " |      Preprocess an image or a batch of images.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.image_processing_utils.ImageProcessingMixin:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Union[bool, NoneType] = None, commit_message: Union[str, NoneType] = None, private: Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '10GB', create_pr: bool = False, safe_serialization: bool = False, **deprecated_kwargs) -> str\n",
      " |      Upload the image processor file to the ğŸ¤— Model Hub while synchronizing a local clone of the repo in\n",
      " |      `repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your image processor to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload image processor\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          use_auth_token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoImageProcessor\n",
      " |      \n",
      " |      image processor = AutoImageProcessor.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the image processor to your namespace with the name \"my-finetuned-bert\".\n",
      " |      image processor.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the image processor to an organization with the name \"my-finetuned-bert\".\n",
      " |      image processor.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      " |      Save an image processor object to the directory `save_directory`, so that it can be re-loaded using the\n",
      " |      [`~image_processing_utils.ImageProcessingMixin.from_pretrained`] class method.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the image processor JSON file will be saved (will be created if it does not exist).\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  to_dict(self) -> Dict[str, Any]\n",
      " |      Serializes this instance to a Python dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Dictionary of all the attributes that make up this image processor instance.\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save this instance to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file in which this image_processor instance's parameters will be saved.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes this instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: String containing all the attributes that make up this feature_extractor instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.image_processing_utils.ImageProcessingMixin:\n",
      " |  \n",
      " |  from_dict(image_processor_dict: Dict[str, Any], **kwargs) from builtins.type\n",
      " |      Instantiates a type of [`~image_processing_utils.ImageProcessingMixin`] from a Python dictionary of parameters.\n",
      " |      \n",
      " |      Args:\n",
      " |          image_processor_dict (`Dict[str, Any]`):\n",
      " |              Dictionary that will be used to instantiate the image processor object. Such a dictionary can be\n",
      " |              retrieved from a pretrained checkpoint by leveraging the\n",
      " |              [`~image_processing_utils.ImageProcessingMixin.to_dict`] method.\n",
      " |          kwargs (`Dict[str, Any]`):\n",
      " |              Additional parameters from which to initialize the image processor object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~image_processing_utils.ImageProcessingMixin`]: The image processor object instantiated from those\n",
      " |          parameters.\n",
      " |  \n",
      " |  from_json_file(json_file: Union[str, os.PathLike]) from builtins.type\n",
      " |      Instantiates a image processor of type [`~image_processing_utils.ImageProcessingMixin`] from the path to a JSON\n",
      " |      file of parameters.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file containing the parameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A image processor of type [`~image_processing_utils.ImageProcessingMixin`]: The image_processor object\n",
      " |          instantiated from that JSON file.\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) from builtins.type\n",
      " |      Instantiate a type of [`~image_processing_utils.ImageProcessingMixin`] from an image processor.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              This can be either:\n",
      " |      \n",
      " |              - a string, the *model id* of a pretrained image_processor hosted inside a model repo on\n",
      " |                huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or\n",
      " |                namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |              - a path to a *directory* containing a image processor file saved using the\n",
      " |                [`~image_processing_utils.ImageProcessingMixin.save_pretrained`] method, e.g.,\n",
      " |                `./my_model_directory/`.\n",
      " |              - a path or url to a saved image processor JSON *file*, e.g.,\n",
      " |                `./my_model_directory/preprocessor_config.json`.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model image processor should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force to (re-)download the image processor files and override the cached versions if\n",
      " |              they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received file. Attempts to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
      " |          use_auth_token (`str` or `bool`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
      " |              the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |      \n",
      " |      \n",
      " |              <Tip>\n",
      " |      \n",
      " |              To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      " |              If `False`, then this function returns just the final image processor object. If `True`, then this\n",
      " |              functions returns a `Tuple(image_processor, unused_kwargs)` where *unused_kwargs* is a dictionary\n",
      " |              consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of\n",
      " |              `kwargs` which has not been used to update `image_processor` and is otherwise ignored.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      " |              specify the folder name here.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              The values in kwargs of any keys which are image processor attributes will be used to override the\n",
      " |              loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is\n",
      " |              controlled by the `return_unused_kwargs` keyword parameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A image processor of type [`~image_processing_utils.ImageProcessingMixin`].\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # We can't instantiate directly the base class *ImageProcessingMixin* so let's show the examples on a\n",
      " |      # derived class: *CLIPImageProcessor*\n",
      " |      image_processor = CLIPImageProcessor.from_pretrained(\n",
      " |          \"openai/clip-vit-base-patch32\"\n",
      " |      )  # Download image_processing_config from huggingface.co and cache.\n",
      " |      image_processor = CLIPImageProcessor.from_pretrained(\n",
      " |          \"./test/saved_model/\"\n",
      " |      )  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*\n",
      " |      image_processor = CLIPImageProcessor.from_pretrained(\"./test/saved_model/preprocessor_config.json\")\n",
      " |      image_processor = CLIPImageProcessor.from_pretrained(\n",
      " |          \"openai/clip-vit-base-patch32\", do_normalize=False, foo=False\n",
      " |      )\n",
      " |      assert image_processor.do_normalize is False\n",
      " |      image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(\n",
      " |          \"openai/clip-vit-base-patch32\", do_normalize=False, foo=False, return_unused_kwargs=True\n",
      " |      )\n",
      " |      assert image_processor.do_normalize is False\n",
      " |      assert unused_kwargs == {\"foo\": False}\n",
      " |      ```\n",
      " |  \n",
      " |  get_image_processor_dict(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n",
      " |      image processor of type [`~image_processor_utils.ImageProcessingMixin`] using `from_dict`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      " |              specify the folder name here.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the image processor object.\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoImageProcessor') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom image processors as the ones\n",
      " |      in the library are already mapped with `AutoImageProcessor `.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoImageProcessor \"`):\n",
      " |              The auto class to register this new image processor with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    }
   ],
   "source": [
    "help(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: remote control, remote\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function open in module PIL.Image:\n",
      "\n",
      "open(fp, mode='r', formats=None)\n",
      "    Opens and identifies the given image file.\n",
      "    \n",
      "    This is a lazy operation; this function identifies the file, but\n",
      "    the file remains open and the actual image data is not read from\n",
      "    the file until you try to process the data (or call the\n",
      "    :py:meth:`~PIL.Image.Image.load` method).  See\n",
      "    :py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n",
      "    \n",
      "    :param fp: A filename (string), pathlib.Path object or a file object.\n",
      "       The file object must implement ``file.read``,\n",
      "       ``file.seek``, and ``file.tell`` methods,\n",
      "       and be opened in binary mode.\n",
      "    :param mode: The mode.  If given, this argument must be \"r\".\n",
      "    :param formats: A list or tuple of formats to attempt to load the file in.\n",
      "       This can be used to restrict the set of formats checked.\n",
      "       Pass ``None`` to try all supported formats. You can print the set of\n",
      "       available formats by running ``python3 -m PIL`` or using\n",
      "       the :py:func:`PIL.features.pilinfo` function.\n",
      "    :returns: An :py:class:`~PIL.Image.Image` object.\n",
      "    :exception FileNotFoundError: If the file cannot be found.\n",
      "    :exception PIL.UnidentifiedImageError: If the image cannot be opened and\n",
      "       identified.\n",
      "    :exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n",
      "       instance is used for ``fp``.\n",
      "    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\n"
     ]
    }
   ],
   "source": [
    "help(Image.open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T06:03:29.040376Z",
     "start_time": "2023-09-18T06:03:29.034397Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/Users/fanke.chang/Downloads/part-00000-305f4cde-2990-42ba-b7b4-794cd590b04b-c000.zstd.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T06:00:32.325130Z",
     "start_time": "2023-09-18T06:00:24.383876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                    index_id                         pp_id  \\\n0    aaPH_Risk73de29Fg0a8Jpr  16PH_Risk1692398997299658752   \n1    2bPH_Risk6yDeyBCqj816JL  2fPH_Risk1811706771754475575   \n2    06PH_RiskBAOEeBSvBGEPdj  57PH_Risk1769234615761748012   \n3    d5PH_Riskx42Gl9f9QNbYzV  6ePH_Risk1740553427303810049   \n4    4bPH_Risk1Qlde8HDyoMM4r  81PH_Risk1727338901422893057   \n..                       ...                           ...   \n989  1aPH_RisklZzyWzTkoBWYe8  77PH_Risk1698258829420460034   \n990  c6PH_Risk60VqMptqj8RoyE  77PH_Risk1809202416337838110   \n991  b6PH_RiskBD3NK2cvBG7WkW  7aPH_Risk1788300164320079897   \n992  d3PH_Risky8a933FWJxY1ev  aePH_Risk1800898969242769442   \n993  9fPH_Riskpd8WLOTyagZyBV  ffPH_Risk1725198235712491521   \n\n                                     formatted_address  \\\n0    Tita'S Store, 82, Paliparan I, Dasmarinas City...   \n1    Potol, Tayabas City, Quezon, 4301 South Luzon,...   \n2    Ground Floor, Waltermart Sucat Expression Stat...   \n3    Poblacion 6, Mamburao, Occidental Mindoro, 510...   \n4    210 Paraan Street, Look 1St, Malolos City, Bul...   \n..                                                 ...   \n989  271 Eucalyptus Street, Lamarea Subdivision, Sa...   \n990  Block 16 Lot 2, Ridgepoint Subdivision, Dalig,...   \n991  Ecoverde Homes, Block 14a Lot 24 Qatar Street,...   \n992  Sitio Lana, Ab-Abut, Piddig, Ilocos Norte, 291...   \n993  25 Edison Street, San Manuel, San Jose Del Mon...   \n\n                                           raw_address  \\\n0                        82 paliparan 1.  Tita's Store   \n1                     Brgy. Ibabang Potol Tayabas City   \n2    Dr.a santos ave.brgy San Isidro ground floor w...   \n3                          MAMBURAO OCCIDENTAL MINDORO   \n4    210 Paraan St. Look 1st Malolos City Bulacan (...   \n..                                                 ...   \n989    271 eucalyptus st lamarea subd san pedro laguna   \n990  Block 16 Lot 2 barangay Prinza,Teresa Rizal Ri...   \n991  Lot 24 Blk 14A Qatar St.,Ecoverde Homes,Brgy. ...   \n992                                         Sitio Lana   \n993                                   25 Edison Street   \n\n                                     address_component  \n0    {\"index_id\": \"16PH_Risk1692398997299658752\", \"...  \n1    {\"index_id\": \"2fPH_Risk1811706771754475575\", \"...  \n2    {\"index_id\": \"57PH_Risk1769234615761748012\", \"...  \n3    {\"index_id\": \"6ePH_Risk1740553427303810049\", \"...  \n4    {\"index_id\": \"81PH_Risk1727338901422893057\", \"...  \n..                                                 ...  \n989  {\"index_id\": \"77PH_Risk1698258829420460034\", \"...  \n990  {\"index_id\": \"77PH_Risk1809202416337838110\", \"...  \n991  {\"index_id\": \"7aPH_Risk1788300164320079897\", \"...  \n992  {\"index_id\": \"aePH_Risk1800898969242769442\", \"...  \n993  {\"index_id\": \"ffPH_Risk1725198235712491521\", \"...  \n\n[994 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_id</th>\n      <th>pp_id</th>\n      <th>formatted_address</th>\n      <th>raw_address</th>\n      <th>address_component</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aaPH_Risk73de29Fg0a8Jpr</td>\n      <td>16PH_Risk1692398997299658752</td>\n      <td>Tita'S Store, 82, Paliparan I, Dasmarinas City...</td>\n      <td>82 paliparan 1.  Tita's Store</td>\n      <td>{\"index_id\": \"16PH_Risk1692398997299658752\", \"...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2bPH_Risk6yDeyBCqj816JL</td>\n      <td>2fPH_Risk1811706771754475575</td>\n      <td>Potol, Tayabas City, Quezon, 4301 South Luzon,...</td>\n      <td>Brgy. Ibabang Potol Tayabas City</td>\n      <td>{\"index_id\": \"2fPH_Risk1811706771754475575\", \"...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>06PH_RiskBAOEeBSvBGEPdj</td>\n      <td>57PH_Risk1769234615761748012</td>\n      <td>Ground Floor, Waltermart Sucat Expression Stat...</td>\n      <td>Dr.a santos ave.brgy San Isidro ground floor w...</td>\n      <td>{\"index_id\": \"57PH_Risk1769234615761748012\", \"...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d5PH_Riskx42Gl9f9QNbYzV</td>\n      <td>6ePH_Risk1740553427303810049</td>\n      <td>Poblacion 6, Mamburao, Occidental Mindoro, 510...</td>\n      <td>MAMBURAO OCCIDENTAL MINDORO</td>\n      <td>{\"index_id\": \"6ePH_Risk1740553427303810049\", \"...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4bPH_Risk1Qlde8HDyoMM4r</td>\n      <td>81PH_Risk1727338901422893057</td>\n      <td>210 Paraan Street, Look 1St, Malolos City, Bul...</td>\n      <td>210 Paraan St. Look 1st Malolos City Bulacan (...</td>\n      <td>{\"index_id\": \"81PH_Risk1727338901422893057\", \"...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>989</th>\n      <td>1aPH_RisklZzyWzTkoBWYe8</td>\n      <td>77PH_Risk1698258829420460034</td>\n      <td>271 Eucalyptus Street, Lamarea Subdivision, Sa...</td>\n      <td>271 eucalyptus st lamarea subd san pedro laguna</td>\n      <td>{\"index_id\": \"77PH_Risk1698258829420460034\", \"...</td>\n    </tr>\n    <tr>\n      <th>990</th>\n      <td>c6PH_Risk60VqMptqj8RoyE</td>\n      <td>77PH_Risk1809202416337838110</td>\n      <td>Block 16 Lot 2, Ridgepoint Subdivision, Dalig,...</td>\n      <td>Block 16 Lot 2 barangay Prinza,Teresa Rizal Ri...</td>\n      <td>{\"index_id\": \"77PH_Risk1809202416337838110\", \"...</td>\n    </tr>\n    <tr>\n      <th>991</th>\n      <td>b6PH_RiskBD3NK2cvBG7WkW</td>\n      <td>7aPH_Risk1788300164320079897</td>\n      <td>Ecoverde Homes, Block 14a Lot 24 Qatar Street,...</td>\n      <td>Lot 24 Blk 14A Qatar St.,Ecoverde Homes,Brgy. ...</td>\n      <td>{\"index_id\": \"7aPH_Risk1788300164320079897\", \"...</td>\n    </tr>\n    <tr>\n      <th>992</th>\n      <td>d3PH_Risky8a933FWJxY1ev</td>\n      <td>aePH_Risk1800898969242769442</td>\n      <td>Sitio Lana, Ab-Abut, Piddig, Ilocos Norte, 291...</td>\n      <td>Sitio Lana</td>\n      <td>{\"index_id\": \"aePH_Risk1800898969242769442\", \"...</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>9fPH_Riskpd8WLOTyagZyBV</td>\n      <td>ffPH_Risk1725198235712491521</td>\n      <td>25 Edison Street, San Manuel, San Jose Del Mon...</td>\n      <td>25 Edison Street</td>\n      <td>{\"index_id\": \"ffPH_Risk1725198235712491521\", \"...</td>\n    </tr>\n  </tbody>\n</table>\n<p>994 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T06:00:38.835689Z",
     "start_time": "2023-09-18T06:00:38.825298Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df['hash_id'] = df['index_id'].apply(lambda x: int(hashlib.sha256(x.encode()).hexdigest(), 16))\n",
    "df.sort_values(['hash_id', 'index_id', 'formatted_address']).to_csv('~/Downloads/cluster_result_0918',index=False,sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T06:05:48.758247Z",
     "start_time": "2023-09-18T06:05:48.693180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "994"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T06:09:05.906520Z",
     "start_time": "2023-09-18T06:09:05.894525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from paddleocr import PPStructure,draw_structure_result,save_structure_res\n",
    "\n",
    "table_engine = PPStructure(show_log=True, image_orientation=True)\n",
    "\n",
    "save_folder = '~/Downloads'\n",
    "img_path = '/Users/fanke.chang/shop/image_filter/negative/test_form.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "result = table_engine(img)\n",
    "save_structure_res(result, save_folder,os.path.basename(img_path).split('.')[0])\n",
    "\n",
    "for line in result:\n",
    "    line.pop('img')\n",
    "    print(line)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T16:10:45.066555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_folder = '/data/home/fanke.chang/shop/product_coding/image_filter'\n",
    "img_path = '/data/home/fanke.chang/shop/product_coding/image_filter/negative/test_tabel.jpg'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "font_path = 'doc/fonts/simfang.ttf' # PaddleOCRä¸‹æä¾›å­—ä½“åŒ…\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "im_show = draw_structure_result(image, result,font_path=font_path)\n",
    "im_show = Image.fromarray(im_show)\n",
    "im_show.save('~/Downloads/result.jpg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['äº§å“åç§°ï¼šå¤§å·åŠ åšåƒåœ¾è¢‹', 'æ¬¾å¼ï¼šå¹³å£å¼']\n",
      "['äº§å“åŠŸèƒ½ï¼š ï¼š æ•´ç†ã€æ”¶çº³', 'ææ–™ï¼šå…¨æ–°PEæ–™']\n",
      "['', '']\n",
      "['é€‚ç”¨åœºåˆï¼šå…¬å¸/å­¦æ ¡/ç‰©ä¸š/ä¿æ´/é¤å…/é…’åº—ç­‰', '']\n",
      "['', '']\n",
      "['è¢‹å­å°ºå¯¸', 'åœ†æ¡¶ç›´å¾„', 'æ–¹æ¡¶å‘¨é•¿', '', 'æ¡¶é«˜']\n",
      "['55*65', 'â‰¤35cm', 'â‰¤110cm', '', 'â‰¤40cm']\n",
      "['60*80', 'â‰¤38cm', '', 'â‰¤120cm', 'â‰¤50cm']\n",
      "['70*80', 'â‰¤44cm', 'â‰¤140cm', '', 'â‰¤50cm']\n",
      "['70*90', 'â‰¤44cm', 'â‰¤140cm', '', 'â‰¤60cm']\n",
      "['80*90', 'â‰¤50cm', 'â‰¤160cm', '', 'â‰¤60cm']\n",
      "['80*100', 'â‰¤50cm', 'â‰¤160cm', '', 'â‰¤70cm']\n",
      "['90*100', 'â‰¤58cm', 'â‰¤180cm', '', 'â‰¤70cm']\n",
      "['90*110', 'â‰¤58cm', 'â‰¤180cm', '', 'â‰¤75cm']\n",
      "['100*110', 'â‰¤63cm', 'â‰¤200cm', '', 'â‰¤75cm']\n",
      "['100*120', 'â‰¤63cm', 'â‰¤200cm', '', 'â‰¤90cm']\n",
      "['120*140', 'â‰¤76cm', 'â‰¤240cm', '', 'â‰¤110cm']\n",
      "['130*140', 'â‰¤82cm', 'â‰¤260cm', '', 'â‰¤110cm']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTMLå†…å®¹\n",
    "html_content = '<html><body><table><tbody><tr><td colspan=\"3\">\\u4ea7\\u54c1\\u540d\\u79f0\\uff1a\\u5927\\u53f7\\u52a0\\u539a\\u5783\\u573e\\u888b</td><td colspan=\"2\">\\u6b3e\\u5f0f\\uff1a\\u5e73\\u53e3\\u5f0f</td></tr><tr><td colspan=\"3\">\\u4ea7\\u54c1\\u529f\\u80fd\\uff1a \\uff1a \\u6574\\u7406\\u3001\\u6536\\u7eb3</td><td colspan=\"2\">\\u6750\\u6599\\uff1a\\u5168\\u65b0PE\\u6599</td></tr><tr><td colspan=\"2\"></td><td colspan=\"3\"></td></tr><tr><td colspan=\"4\">\\u9002\\u7528\\u573a\\u5408\\uff1a\\u516c\\u53f8/\\u5b66\\u6821/\\u7269\\u4e1a/\\u4fdd\\u6d01/\\u9910\\u5385/\\u9152\\u5e97\\u7b49</td><td></td></tr><tr><td colspan=\"4\"></td><td></td></tr><tr><td>\\u888b\\u5b50\\u5c3a\\u5bf8</td><td>\\u5706\\u6876\\u76f4\\u5f84</td><td>\\u65b9\\u6876\\u5468\\u957f</td><td></td><td>\\u6876\\u9ad8</td></tr><tr><td>55*65</td><td>\\u226435cm</td><td>\\u2264110cm</td><td></td><td>\\u226440cm</td></tr><tr><td>60*80</td><td>\\u226438cm</td><td></td><td>\\u2264120cm</td><td>\\u226450cm</td></tr><tr><td>70*80</td><td>\\u226444cm</td><td>\\u2264140cm</td><td></td><td>\\u226450cm</td></tr><tr><td>70*90</td><td>\\u226444cm</td><td>\\u2264140cm</td><td></td><td>\\u226460cm</td></tr><tr><td>80*90</td><td>\\u226450cm</td><td>\\u2264160cm</td><td></td><td>\\u226460cm</td></tr><tr><td>80*100</td><td>\\u226450cm</td><td>\\u2264160cm</td><td></td><td>\\u226470cm</td></tr><tr><td>90*100</td><td>\\u226458cm</td><td>\\u2264180cm</td><td></td><td>\\u226470cm</td></tr><tr><td>90*110</td><td>\\u226458cm</td><td>\\u2264180cm</td><td></td><td>\\u226475cm</td></tr><tr><td>100*110</td><td>\\u226463cm</td><td>\\u2264200cm</td><td></td><td>\\u226475cm</td></tr><tr><td>100*120</td><td>\\u226463cm</td><td>\\u2264200cm</td><td></td><td>\\u226490cm</td></tr><tr><td>120*140</td><td>\\u226476cm</td><td>\\u2264240cm</td><td></td><td>\\u2264110cm</td></tr><tr><td>130*140</td><td>\\u226482cm</td><td>\\u2264260cm</td><td></td><td>\\u2264110cm</td></tr></tbody></table></body></html>'  # è¯·å°†å…¶æ›¿æ¢ä¸ºæ‚¨çš„HTMLå†…å®¹\n",
    "\n",
    "# ä½¿ç”¨Beautiful Soupè§£æHTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# æ‰¾åˆ°è¡¨æ ¼å…ƒç´ \n",
    "table = soup.find('table')\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªç”¨äºå­˜å‚¨è¡¨æ ¼æ•°æ®çš„åˆ—è¡¨\n",
    "table_data = []\n",
    "\n",
    "# éå†è¡¨æ ¼çš„æ¯ä¸€è¡Œå’Œæ¯ä¸€åˆ—ï¼Œå¹¶æå–æ•°æ®\n",
    "for row in table.find_all('tr'):\n",
    "    row_data = []\n",
    "    for cell in row.find_all(['td', 'th']):\n",
    "        row_data.append(cell.get_text())\n",
    "    table_data.append(row_data)\n",
    "\n",
    "# æ‰“å°æå–çš„è¡¨æ ¼æ•°æ®\n",
    "for row in table_data:\n",
    "    print(row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:30:27.739022Z",
     "start_time": "2023-09-21T16:30:27.638115Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T06:45:47.941303Z",
     "start_time": "2023-09-27T06:45:47.021023Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                    master_place_id                       return_id  \\\n0    1663666425_2022-08-05391383098  1663666441_2022-08-05196581694   \n1    1663666425_2022-08-05391383098     1671900013_2022-12-24679858   \n2    1663666425_2022-08-05391383098  1663667278_2022-08-05376893676   \n3    1663666425_2022-08-05391383098  1663666536_2022-08-05104055052   \n4    1663666425_2022-08-05391383098  1663666764_2022-08-05303210712   \n..                              ...                             ...   \n393  1663666742_2022-08-05401702216  1663666556_2022-08-05321990223   \n394  1663666742_2022-08-05401702216  1663665819_2022-08-05222470276   \n395  1663666742_2022-08-05401702216  1663666945_2022-08-05204161522   \n396  1663666742_2022-08-05401702216  1663667130_2022-08-05283053252   \n397  1663666742_2022-08-05401702216  1663667033_2022-08-05132446840   \n\n                    expect_place_id  \\\n0    1663666150_2022-08-05208653345   \n1    1663666150_2022-08-05208653345   \n2    1663666150_2022-08-05208653345   \n3    1663666150_2022-08-05208653345   \n4    1663666150_2022-08-05208653345   \n..                              ...   \n393   1663665799_2022-08-0514452076   \n394   1663665799_2022-08-0514452076   \n395   1663665799_2022-08-0514452076   \n396   1663665799_2022-08-0514452076   \n397   1663665799_2022-08-0514452076   \n\n                              master_formatted_address  \\\n0    Matahari Binjai Supermall, Jalan L Soekarno Ha...   \n1    Matahari Binjai Supermall, Jalan L Soekarno Ha...   \n2    Matahari Binjai Supermall, Jalan L Soekarno Ha...   \n3    Matahari Binjai Supermall, Jalan L Soekarno Ha...   \n4    Matahari Binjai Supermall, Jalan L Soekarno Ha...   \n..                                                 ...   \n393  RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...   \n394  RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...   \n395  RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...   \n396  RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...   \n397  RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...   \n\n                              return_formatted_address  \\\n0    Matahari Binjai Supermall, Jalan Soekarno Hatt...   \n1    Matahari Binjai Supermall, Jalan Soekarno Hatt...   \n2    Matahari Binjai Supermall, Jalan Soekarno Hatt...   \n3    Matahari Binjai Supermall, Jalan Soekarno Hatt...   \n4    Matahari Binjai Supermall, Jalan Soekarno Hatt...   \n..                                                 ...   \n393  Jalan Kedungmegarih, RT.3/RW.1, Dusun Kedungdo...   \n394  Gang Masjid, RT.3/RW.1, Desa Dumpiagung, Kemba...   \n395  Gang Masjid, RT.3/RW.1, Desa Lopang, Kembangba...   \n396  Pak Tampang, RT.3/RW.1, Desa Pelang, Kembangba...   \n397  Blok 1, RT.3/RW.1, Dusun Kedongori, Desa Kedun...   \n\n                              expect_formatted_address  es_score  \n0    Binjai Supermall, Jalan Soekarno Hatta, Tanah ...  5.919355  \n1    Binjai Supermall, Jalan Soekarno Hatta, Tanah ...  5.919355  \n2    Binjai Supermall, Jalan Soekarno Hatta, Tanah ...  5.919355  \n3    Binjai Supermall, Jalan Soekarno Hatta, Tanah ...  5.919355  \n4    Binjai Supermall, Jalan Soekarno Hatta, Tanah ...  5.919355  \n..                                                 ...       ...  \n393  RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...       1.6  \n394  RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...       1.6  \n395  RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...       1.6  \n396  RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...       1.6  \n397  RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...       1.6  \n\n[398 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>master_place_id</th>\n      <th>return_id</th>\n      <th>expect_place_id</th>\n      <th>master_formatted_address</th>\n      <th>return_formatted_address</th>\n      <th>expect_formatted_address</th>\n      <th>es_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1663666425_2022-08-05391383098</td>\n      <td>1663666441_2022-08-05196581694</td>\n      <td>1663666150_2022-08-05208653345</td>\n      <td>Matahari Binjai Supermall, Jalan L Soekarno Ha...</td>\n      <td>Matahari Binjai Supermall, Jalan Soekarno Hatt...</td>\n      <td>Binjai Supermall, Jalan Soekarno Hatta, Tanah ...</td>\n      <td>5.919355</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1663666425_2022-08-05391383098</td>\n      <td>1671900013_2022-12-24679858</td>\n      <td>1663666150_2022-08-05208653345</td>\n      <td>Matahari Binjai Supermall, Jalan L Soekarno Ha...</td>\n      <td>Matahari Binjai Supermall, Jalan Soekarno Hatt...</td>\n      <td>Binjai Supermall, Jalan Soekarno Hatta, Tanah ...</td>\n      <td>5.919355</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1663666425_2022-08-05391383098</td>\n      <td>1663667278_2022-08-05376893676</td>\n      <td>1663666150_2022-08-05208653345</td>\n      <td>Matahari Binjai Supermall, Jalan L Soekarno Ha...</td>\n      <td>Matahari Binjai Supermall, Jalan Soekarno Hatt...</td>\n      <td>Binjai Supermall, Jalan Soekarno Hatta, Tanah ...</td>\n      <td>5.919355</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1663666425_2022-08-05391383098</td>\n      <td>1663666536_2022-08-05104055052</td>\n      <td>1663666150_2022-08-05208653345</td>\n      <td>Matahari Binjai Supermall, Jalan L Soekarno Ha...</td>\n      <td>Matahari Binjai Supermall, Jalan Soekarno Hatt...</td>\n      <td>Binjai Supermall, Jalan Soekarno Hatta, Tanah ...</td>\n      <td>5.919355</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1663666425_2022-08-05391383098</td>\n      <td>1663666764_2022-08-05303210712</td>\n      <td>1663666150_2022-08-05208653345</td>\n      <td>Matahari Binjai Supermall, Jalan L Soekarno Ha...</td>\n      <td>Matahari Binjai Supermall, Jalan Soekarno Hatt...</td>\n      <td>Binjai Supermall, Jalan Soekarno Hatta, Tanah ...</td>\n      <td>5.919355</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>1663666742_2022-08-05401702216</td>\n      <td>1663666556_2022-08-05321990223</td>\n      <td>1663665799_2022-08-0514452076</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...</td>\n      <td>Jalan Kedungmegarih, RT.3/RW.1, Dusun Kedungdo...</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>1663666742_2022-08-05401702216</td>\n      <td>1663665819_2022-08-05222470276</td>\n      <td>1663665799_2022-08-0514452076</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...</td>\n      <td>Gang Masjid, RT.3/RW.1, Desa Dumpiagung, Kemba...</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>1663666742_2022-08-05401702216</td>\n      <td>1663666945_2022-08-05204161522</td>\n      <td>1663665799_2022-08-0514452076</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...</td>\n      <td>Gang Masjid, RT.3/RW.1, Desa Lopang, Kembangba...</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>1663666742_2022-08-05401702216</td>\n      <td>1663667130_2022-08-05283053252</td>\n      <td>1663665799_2022-08-0514452076</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...</td>\n      <td>Pak Tampang, RT.3/RW.1, Desa Pelang, Kembangba...</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>1663666742_2022-08-05401702216</td>\n      <td>1663667033_2022-08-05132446840</td>\n      <td>1663665799_2022-08-0514452076</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Sido Mukti, K...</td>\n      <td>Blok 1, RT.3/RW.1, Dusun Kedongori, Desa Kedun...</td>\n      <td>RT.3/RW.1, Dsn. Kedungglonggong, Ds. Sidomukti...</td>\n      <td>1.6</td>\n    </tr>\n  </tbody>\n</table>\n<p>398 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T10:31:38.362908Z",
     "start_time": "2023-09-27T10:31:38.350038Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
